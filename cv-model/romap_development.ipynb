{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "494d983f",
   "metadata": {},
   "source": [
    "# Remote inspection with machine learning at the edge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f15ce0e",
   "metadata": {},
   "source": [
    "This notebook builds a machine learning (ML) model to perform remote inspection at the edge. The model is developed to run on a device at the edge. The model makes predictions based on thermal images captured at the edge by the device. The model is built using the Yolov4 library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf70b807",
   "metadata": {},
   "source": [
    "## Yolov4 Pytorch 1.7 for Edge Devices with Amazon SageMaker\n",
    "\n",
    "Amazon SageMaker is a fully managed machine learning service. With SageMaker, data scientists and developers can quickly and easily build and train machine learning models, and then directly deploy them into a production-ready hosted environment. It provides an integrated Jupyter authoring notebook instance for easy access to your data sources for exploration and analysis, so you don't have to manage servers. It also provides common machine learning algorithms that are optimized to run efficiently against extremely large data in a distributed environment. With native support for bring-your-own-algorithms and frameworks, SageMaker offers flexible distributed training options that adjust to your specific workflows.\n",
    "\n",
    "SageMaker also offers capabilities to prepare models for deployment at the edge. [SageMaker Neo](https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html) is a capability of Amazon SageMaker that enables machine learning models to train once and run anywhere in the cloud and at the edge and [Amazon SageMaker Edge Manager](https://docs.aws.amazon.com/sagemaker/latest/dg/edge.html) provides model management for edge devices so you can optimize, secure, monitor, and maintain machine learning models on fleets of edge devices such as smart cameras, robots, personal computers, and mobile devices.\n",
    "\n",
    "In this notebook we'll train a [**Yolov4**](https://github.com/WongKinYiu/PyTorch_YOLOv4) model on Pytorch using Amazon SageMaker to draw bounding boxes around images and then, compile and package it so that it can be deployed on an edge device(in this case, a [Jetson Xavier](https://developer.nvidia.com/jetpack-sdk-441-archive))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1108d7a0",
   "metadata": {},
   "source": [
    "## 1) Pre-requisites\n",
    "\n",
    "Let us start with setting up the pre-requisites for this notebook. First, we will sagemaker and other related libs and then set up the role and buckets and some variables. Note that, we are also specifying the size of the image and also the model size taken as Yolov4s where s stand for small. Check out the [github doc of yolov4](https://github.com/WongKinYiu/PyTorch_YOLOv4) to understand how model sizes differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f177f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "\n",
    "from matplotlib import pyplot as plt  # for image display\n",
    "import matplotlib.patches as patches  # for image display\n",
    "from PIL import Image as pil_image    # PIL is the Python Imaging Library for image display\n",
    "import random                         # for image display\n",
    "from pathlib import Path              # for image display\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session=sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "img_size=640\n",
    "model_type='tiny' # tiny or ''\n",
    "model_name='yolov4'if model_type=='' else f\"yolov4-{model_type}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b97870",
   "metadata": {},
   "source": [
    "### 1.1) Download a public implementation of Yolov4 for Pytorch\n",
    "\n",
    "Now, we will download the PyTorch implementation of Yolov4 from this [repository](https://github.com/WongKinYiu/PyTorch_YOLOv4) which is authored by Wong Kin Yiu. We will place it in a local directory `yolov4` and apply a patch for cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a2a535",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('yolov4'):\n",
    "    !git clone https://github.com/WongKinYiu/PyTorch_YOLOv4 yolov4\n",
    "    !cd yolov4 && git checkout 3c42cbd1b0fa28ad19436d01e0e240404463ff80 && git apply ../mish.patch\n",
    "    !echo 'tensorboard' > yolov4/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d737ca7b",
   "metadata": {},
   "source": [
    "### 1.2) Image display constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860ccabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './panelscoco/images/'\n",
    "image_extension = '.jpg'\n",
    "label_path = './panelscoco/labels/'\n",
    "label_extension = '.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208345b7",
   "metadata": {},
   "source": [
    "### 1.3) Function to display image with all bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5836daf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image_with_bounding_boxes (image, boxes):\n",
    "    # function to display an image with its bounding boxes\n",
    "    #\n",
    "    # image is a Python Image Library image object holding image\n",
    "    # boxes is a np.array holding bounding boxes\n",
    "    #    in COCO format bounding box annotation\n",
    "    #    [class, x_center, y_center, width, height]\n",
    "    #    in dimensions as a fraction of the whole\n",
    "\n",
    "    def add_bounding_box_to_image (axis, box, width, height):\n",
    "        # function to add a single bounding box to an image\n",
    "        #\n",
    "        # axis is MatLibPlot axis object\n",
    "        # box is np array holding dimensions for a single bounding box\n",
    "        #    in COCO format bounding box annotation\n",
    "        #    [class, x_center, y_center, width, height]\n",
    "        #    in dimensions as a fraction of the whole\n",
    "        # width is image width in pixels\n",
    "        # height is image height in pixels\n",
    "        \n",
    "        # Create a Rectangle patch for the box\n",
    "        rect = patches.Rectangle(((box[1]-box[3])*width, (box[2]-box[4])*height),\n",
    "                                 box[3]*width,\n",
    "                                 box[4]*height,\n",
    "                                 edgecolor='r',\n",
    "                                 facecolor='none')\n",
    "        axis.add_patch(rect)\n",
    "        \n",
    "    # get image width & height\n",
    "    width, height = image.size\n",
    "\n",
    "    # Create the figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # set the image\n",
    "    ax.imshow(np.array(image))    # convert image to np.array for use by MatLibPlot imshow\n",
    "    \n",
    "    # loop through boxes to add each bounding box to the image\n",
    "    # get number of boxes\n",
    "    number_of_boxes = boxes.shape[0]    # index 0 returns first dimension, index 1 returns second dimension, etc.\n",
    "    for counter in range(number_of_boxes):    # loops from 0 to (number_of_boxes - 1)\n",
    "        # get a single box from boxes\n",
    "        box = boxes [counter]\n",
    "        # add the bounding box to the image\n",
    "        add_bounding_box_to_image (ax, box, width, height)\n",
    "    \n",
    "    # show the image with the bounding boxes\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d791e73",
   "metadata": {},
   "source": [
    "## 2) Data engineering (inspection, preparation, and analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41ca646",
   "metadata": {},
   "source": [
    "### 2.1) Data inspection  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a338fe77",
   "metadata": {},
   "source": [
    "#### 2.1.1) Label format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d11ac1a",
   "metadata": {},
   "source": [
    "Lets look at the label format used for the dataset. It should follow the same standard as COCO.\n",
    "\n",
    "If you need an example of the COCO format standard, a sample dataset is available [coco128](https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip). Use this code to download it.\n",
    "\n",
    "```\n",
    "if not os.path.exists('coco128'):\n",
    "    !wget -q https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip\n",
    "    !unzip -q coco128.zip && rm -f coco128.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fffa9b5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('BBoxes annotation')\n",
    "print('class x_center y_center width height')\n",
    "!head panelscoco/labels/0_1-0_thermal.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61d5768",
   "metadata": {},
   "source": [
    "#### 2.1.2) Randomly select four images and display them with there bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae978b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for counter in range(4):\n",
    "    # randomly get a filename with extension but without the path\n",
    "    image_filename = random.choice(os.listdir(image_path))\n",
    "    # get the stem of the filename\n",
    "    file_stem = Path(image_filename).stem\n",
    "    print(\"stem\", file_stem)\n",
    "    image_file = image_path + file_stem + image_extension    # create the full path to the image \n",
    "    label_file = label_path + file_stem + label_extension    # create the full path to the bounding box labels for the image\n",
    "    \n",
    "    if Path(label_path + file_stem + label_extension).stat().st_size > 0:    # ignore if the labels are empty\n",
    "        # image as a np.array holding image\n",
    "        image = pil_image.open(image_file)\n",
    "        # boxes as a np.array holding bounding boxes\n",
    "        boxes = np.genfromtxt(label_file, delimiter=' ')\n",
    "        display_image_with_bounding_boxes (image, boxes)\n",
    "    else:\n",
    "        print(\"... has no bounding box labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc74f792",
   "metadata": {},
   "source": [
    "### 2.2) Data preparation  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0616939b",
   "metadata": {},
   "source": [
    "#### 2.2.1) Upload the dataset to S3\n",
    "\n",
    "The dataset is uploaded into an S3 bucket. The S3 bucket was created outside of this notebook. \n",
    "\n",
    "The training and validation dataset S3 locations are defined here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix='data/panelscoco'\n",
    "!rm -f panelscoco/labels/train2017.cache\n",
    "train_path = sagemaker_session.upload_data('panelscoco', key_prefix=f'{prefix}/train')\n",
    "val_path = sagemaker_session.upload_data('panelscoco', key_prefix=f'{prefix}/val')\n",
    "print(train_path, val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba916eec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!aws s3 ls $train_path/images/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f465a570",
   "metadata": {},
   "source": [
    "## 3) Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f35f43a",
   "metadata": {},
   "source": [
    "### 3.1) Prepare a Python script that will be the entrypoint of the training process\n",
    "\n",
    "Now, we will create a training script to train the Yolov4 model. The training script will wrap the original training scripts and expose the parameters to SageMaker Estimator. The script accepts different arguments which will control the training process. In this specific case number of classes=1 (CELL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae100d41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile yolov4/sagemaker_train.py\n",
    "import sys\n",
    "import subprocess\n",
    "## We need to remove smdebug to avoid the Hook bug https://github.com/awslabs/sagemaker-debugger/issues/401\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"smdebug\"])\n",
    "import os\n",
    "import yaml\n",
    "import argparse\n",
    "import torch\n",
    "import shutil\n",
    "import urllib\n",
    "from models.models import Darknet\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--num-classes', type=int, default=1, help='Number of classes')\n",
    "    parser.add_argument('--img-size', type=int, default=640, help='Size of the image')\n",
    "    parser.add_argument('--epochs', type=int, default=1, help='Number of epochs')\n",
    "    parser.add_argument('--batch-size', type=int, default=16, help='Batch size')\n",
    "    parser.add_argument('--adam', action='store_true', help='use torch.optim.Adam() optimizer')\n",
    "    parser.add_argument('--pretrained', action='store_true', help='use pretrained model')\n",
    "    \n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ[\"SM_MODEL_DIR\"], help='Trained model dir')\n",
    "    parser.add_argument('--train', type=str, default=os.environ[\"SM_CHANNEL_TRAIN\"], help='Train path')\n",
    "    parser.add_argument('--train-suffix', type=str, default='', help='Train path suffix')\n",
    "    parser.add_argument('--validation', type=str, default=os.environ[\"SM_CHANNEL_VALIDATION\"], help='Validation path')\n",
    "    parser.add_argument('--validation-suffix', type=str, default='', help='Validation path suffix')\n",
    "    \n",
    "    parser.add_argument('--model-type', type=str, choices=['', 'tiny'], default=\"\", help='Model type')\n",
    "    \n",
    "    # hyperparameters\n",
    "    with open('data/hyp.scratch.yaml', 'r') as f:\n",
    "        hyperparams = yaml.load(f, Loader=yaml.FullLoader)    \n",
    "    for k,v in hyperparams.items():\n",
    "        parser.add_argument(f\"--{k.replace('_', '-')}\", type=float, default=v)\n",
    "    \n",
    "    args,unknown = parser.parse_known_args()\n",
    "    \n",
    "    base_path=os.path.dirname(__file__)\n",
    "    project_dir = os.environ[\"SM_OUTPUT_DATA_DIR\"]\n",
    "\n",
    "    # prepare the hyperparameters metadat\n",
    "    with open(os.path.join(base_path,'data', 'hyp.custom.yaml'), 'w' ) as y:\n",
    "        y.write(yaml.dump({h:vars(args)[h] for h in hyperparams.keys()}))\n",
    "\n",
    "    # prepare the training data metadata\n",
    "    with open(os.path.join(base_path,'data', 'custom.yaml'), 'w') as y:\n",
    "        y.write(yaml.dump({            \n",
    "            'names': [f'class_{i}' for i in range(args.num_classes)],\n",
    "            'train': os.path.join(args.train, args.train_suffix),\n",
    "            'val': os.path.join(args.validation, args.validation_suffix),\n",
    "            'nc': args.num_classes\n",
    "        }))\n",
    "    model_name = \"yolov4\" if len(args.model_type) == 0 else f\"yolov4-{args.model_type}\"\n",
    "    # run the training script\n",
    "    weights_file=''\n",
    "    if args.pretrained:\n",
    "        weights_file = f'weights/{model_name}.weights'\n",
    "        urllib.request.urlretrieve(\n",
    "            f'https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/{model_name}.weights',\n",
    "            weights_file\n",
    "        )\n",
    "\n",
    "    train_cmd = [\n",
    "        sys.executable, os.path.join(base_path,'train.py'),\n",
    "        \"--data\", \"custom.yaml\",\n",
    "        \"--hyp\", \"hyp.custom.yaml\",\n",
    "        \"--cfg\", f\"cfg/{model_name}.cfg\",\n",
    "        \"--img\", str(args.img_size),\n",
    "        \"--batch\", str(args.batch_size),\n",
    "        \"--epochs\", str(args.epochs),\n",
    "        \"--logdir\", project_dir,\n",
    "        \"--weights\", weights_file\n",
    "    ]\n",
    "    if args.adam: train_cmd.append(\"--adam\")\n",
    "    subprocess.check_call(train_cmd)\n",
    "        \n",
    "    # tracing and saving the model\n",
    "    inp = torch.rand(1, 3, args.img_size, args.img_size).cpu()\n",
    "    ckpt = torch.load(os.path.join(project_dir, 'exp0', 'weights', 'best.pt'), map_location='cpu')\n",
    "    model = Darknet(f\"cfg/{model_name}.cfg\").cpu()\n",
    "    # do not invoke .eval(). we don't need the detection layer\n",
    "    model.load_state_dict(ckpt['model'], strict=False)\n",
    "    p = model(inp)\n",
    "    model_trace = torch.jit.trace(model, inp)\n",
    "    model_trace.save(os.path.join(args.model_dir, 'model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545e123d",
   "metadata": {},
   "source": [
    "### 3.2) Prepare the SageMaker Estimator to train the model\n",
    "\n",
    "Now it's time to create an Estimater to train the model with the training script created earlier. We are using Pytorch estimator and supplying other arguments in the estimator. Note that we are supplying the `source_dir` so that sagemaker can pick up the training script and other related files from there. Once the estimator is ready, we start the training using the `.fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c16af03",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    'sagemaker_train.py',\n",
    "    source_dir='yolov4',\n",
    "    framework_version='1.7',\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    instance_type='ml.p3.2xlarge',    \n",
    "    instance_count=1,\n",
    "    py_version='py3', \n",
    "    hyperparameters={\n",
    "        'epochs': 20, # at least 2 epochs\n",
    "        'batch-size': 8,\n",
    "        'lr0': 0.0001,\n",
    "        \n",
    "        'pretrained': True, # transfer learning\n",
    "        'num-classes': 1,\n",
    "        'img-size': img_size,\n",
    "        'model-type': model_type,\n",
    "        \n",
    "        # the final path needs to point to the images dir\n",
    "        'train-suffix': 'images',\n",
    "        'validation-suffix': 'images'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa75c628",
   "metadata": {},
   "source": [
    "### 3.3) Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b5090a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.fit({'train': train_path, 'validation': val_path})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3ba4d9",
   "metadata": {},
   "source": [
    "#### 3.3.1) Get the location of the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396ca56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_uri=f'{estimator.output_path}{estimator.latest_training_job.name}/output/model.tar.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0ff371",
   "metadata": {},
   "source": [
    "## 4) Compile your trained model for the edge device\n",
    "\n",
    "Once the model has been trained, we need to compile the model using SageMaker Neo. This step is needed to compile the model for the specific hardware on which this model will be deployed. \n",
    "\n",
    "In this notebook, we will compile a model for [Jetson Xavier Jetpack 4.4.1](https://developer.nvidia.com/jetpack-sdk-441-archive). \n",
    "\n",
    "In case, you want to compile for a different hardware platform, just change the parameters bellow to adjust the target to your own edge device. Also, note, that if you dont have GPU available on the hardware device, then you can comment the `Accelerator` key:value in the `OutputConfig`.\n",
    "\n",
    "The below cell also calls the `describe_compilation_job` API in a loop to wait for the compilation job to complete. In actual applications, it is advisable to setup a cloudwatch event which can notify OR execute the next steps once the compilation job is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bc57a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "sm_client = boto3.client('sagemaker')\n",
    "compilation_job_name = f'{model_name}-pytorch-{int(time.time()*1000)}'\n",
    "sm_client.create_compilation_job(\n",
    "    CompilationJobName=compilation_job_name,\n",
    "    RoleArn=role,\n",
    "    InputConfig={\n",
    "        'S3Uri': s3_uri,\n",
    "        'DataInputConfig': f'{{\"input\": [1,3,{img_size},{img_size}]}}',\n",
    "        'Framework': 'PYTORCH'\n",
    "    },\n",
    "    OutputConfig={\n",
    "        'S3OutputLocation': f's3://{sagemaker_session.default_bucket()}/{model_name}-pytorch/optimized/',\n",
    "        'TargetPlatform': { \n",
    "            'Os': 'LINUX', \n",
    "            'Arch': 'X86_64', # change this to X86_64 if you need o leave ARM64 for Jetson\n",
    "            #'Accelerator': 'NVIDIA'  # comment this if you don't have an Nvidia GPU\n",
    "        },\n",
    "        # Comment or change the following line depending on your edge device\n",
    "        # Jetson Xavier: sm_72; Jetson Nano: sm_53\n",
    "        #'CompilerOptions': '{\"trt-ver\": \"7.1.3\", \"cuda-ver\": \"10.2\", \"gpu-code\": \"sm_53\"}' # Jetpack 4.4.1 Comment fo Ec2\n",
    "    },\n",
    "    StoppingCondition={ 'MaxRuntimeInSeconds': 900 }\n",
    ")\n",
    "while True:\n",
    "    resp = sm_client.describe_compilation_job(CompilationJobName=compilation_job_name)    \n",
    "    if resp['CompilationJobStatus'] in ['STARTING', 'INPROGRESS']:\n",
    "        print('Running...')\n",
    "    else:\n",
    "        print(resp['CompilationJobStatus'], compilation_job_name)\n",
    "        break\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4492cb",
   "metadata": {},
   "source": [
    "## 5) Create a SageMaker Edge Manager packaging job\n",
    "\n",
    "Once the model has been compiled, it is time to create an edge manager packaging job. Packaging job take SageMaker Neo–compiled models and make any changes necessary to deploy the model with the inference engine, Edge Manager agent.\n",
    "\n",
    "We need to provide the name used for the Neo compilation job, a name for the packaging job, a role ARN, a name for the model, a model version, and the Amazon S3 bucket URI for the output of the packaging job. Note that Edge Manager packaging job names are case-sensitive.\n",
    "\n",
    "\n",
    "The below cell also calls the `describe_edge_packaging_job` API in a loop to wait for the packaging job to complete. In actual applications, it is advisable to setup a cloudwatch event which can notify OR execute the next steps once the compilation job is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fd899c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "model_version = '1.0'\n",
    "edge_packaging_job_name=f'{model_name}-pytorch-{int(time.time()*1000)}'\n",
    "resp = sm_client.create_edge_packaging_job(\n",
    "    EdgePackagingJobName=edge_packaging_job_name,\n",
    "    CompilationJobName=compilation_job_name,\n",
    "    ModelName=model_name,\n",
    "    ModelVersion=model_version,\n",
    "    RoleArn=role,\n",
    "    OutputConfig={\n",
    "        'S3OutputLocation': f's3://{bucket_name}/{model_name}'\n",
    "    }\n",
    ")\n",
    "while True:\n",
    "    resp = sm_client.describe_edge_packaging_job(EdgePackagingJobName=edge_packaging_job_name)    \n",
    "    if resp['EdgePackagingJobStatus'] in ['STARTING', 'INPROGRESS']:\n",
    "        print('Running...')\n",
    "    else:\n",
    "        print(resp['EdgePackagingJobStatus'], compilation_job_name)\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfd58552",
   "metadata": {},
   "source": [
    "## 6) Pre processing + Post processing code\n",
    "After compiling your model, it's time to prepare the application that will use it. In the image bellow you can see the operators that are used by the last layer **YOLOLayer**. When in evaluation mode, this layer applies some operations to merge the two outputs of the network and prepare the predictions for the **Non Maximum Suppression**. In training model, you just have the two raw outputs. \n",
    "\n",
    "Given we're using a pruned version (training mode) of the network, you need to apply some post processing code to your predictions.\n",
    "\n",
    "<table style=\"border: 1px solid black; border-collapse: collapse;\" border=3 cellpadding=0 cellspacing=0>\n",
    "    <tr style=\"border: 1px solid black;\">\n",
    "        <td style=\"text-align: center; border-right: 1px solid;\" align=\"center\"><b>WITH DETECTION (evaluation mode)</b></td>\n",
    "        <td style=\"text-align: center;\" align=\"center\"><b>NO DETECTION (training mode)</b></td>\n",
    "    </tr>    \n",
    "    <tr style=\"border: 1px solid black; border-right: 1px solid;\">\n",
    "        <td width=\"75%\" style=\"border-right: 1px solid;\">\n",
    "            <img src=\"imgs/yolov4_detection.png\"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"imgs/yolov4_no_detection.png\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "All the operations required by this process can be found in the script **[Utils](utils.py)**.\n",
    "#### WARNING: run the next cell and copy the output to your **utils.py** before running the application. It is necessary to adjust the anchors and scores for the correct yolov4 version\n",
    "\n",
    "\n",
    "Here it is an example of how to use the code:\n",
    "```Python\n",
    "import utils\n",
    "\n",
    "### your code here\n",
    "### def predict(model, x):...\n",
    "\n",
    "confidence_treshold=0.1\n",
    "# Read the image using OpenCV\n",
    "img = cv2.imread('dog.jpg')\n",
    "# Convert the image to the expected network input\n",
    "x = utils.preprocess_img(img, img_size=416)\n",
    "# Run the model and get the predictions\n",
    "preds = predict(model, x)\n",
    "# Convert the predictions into Detections(bounding boxes, scores and class ids)\n",
    "detections = utils.detect(preds, 0.25, 0.5, True)\n",
    "# Iterate over the detections and do what you need to do\n",
    "for top_left_corner,bottom_right_corner, conf, class_id in detections:\n",
    "    if confidence_treshold < 0.1: continue\n",
    "    print( f\"bbox: {top_left_corner},{bottom_right_corner}, score: {conf}, class_id: {class_id}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4994b1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code parses the .cfg file of the version you're using and\n",
    "## Prints the correct anchors/stride. Copy the output to the 'utils.py' file used by your application\n",
    "import numpy as np\n",
    "\n",
    "cfg_filename=f'yolov4/cfg/{model_name}.cfg'\n",
    "yolo_layer=False\n",
    "yolo_layers = []\n",
    "\n",
    "for i in open(cfg_filename, 'r').readlines(): # read the .cfg file\n",
    "    i = i.strip()\n",
    "    if len(i) == 0 or i.startswith('#'): continue # ignore empty lines and comments\n",
    "    elif i.startswith('[') and i.endswith(']'): # header\n",
    "        if i.lower().replace(' ', '') == '[yolo]':\n",
    "            yolo_layer = True # yolo layer\n",
    "            yolo_layers.append({})\n",
    "    elif yolo_layer: # properties of the layer\n",
    "        k,v = [a.strip() for a in i.split('=')] # split line into key, value\n",
    "        if k == 'anchors': # parse anchors\n",
    "            anchors = np.array([int(j.strip()) for j in v.split(',')])\n",
    "            yolo_layers[-1]['anchors'] = anchors.reshape((len(anchors)//2, 2))\n",
    "        elif k == 'mask': # parse mask\n",
    "            yolo_layers[-1]['mask'] = np.array([int(j.strip()) for j in v.split(',')])\n",
    "\n",
    "stride = [8, 16, 32, 64, 128]  # P3, P4, P5, P6, P7 strides\n",
    "if model_type == 'tiny':  # P5, P4, P3 strides\n",
    "    stride = [32, 16, 8]\n",
    "\n",
    "print(\"##### Copy the following lines to your util.py\")\n",
    "print(f\"anchors = {[l['anchors'][l['mask']].tolist() for l in yolo_layers]}\")\n",
    "print(f\"stride = {stride}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbf03dc",
   "metadata": {},
   "source": [
    "### Done !!\n",
    "\n",
    "And we are done with all the steps needed to prepare the model for deploying to edge. The model package is avaialble in S3 and can be taken from there to deploy it to edge device. Now you need to move over to your edge device and download and setup edge manager agent(runtime), model and other related artifacts on the device. Please check out the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/edge.html) for detailed steps."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
