{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yolov4 Pytorch 1.7 for Edge Devices with Amazon SageMaker\n",
    "\n",
    "\n",
    "Amazon SageMaker is a fully managed machine learning service. With SageMaker, data scientists and developers can quickly and easily build and train machine learning models, and then directly deploy them into a production-ready hosted environment. It provides an integrated Jupyter authoring notebook instance for easy access to your data sources for exploration and analysis, so you don't have to manage servers. It also provides common machine learning algorithms that are optimized to run efficiently against extremely large data in a distributed environment. With native support for bring-your-own-algorithms and frameworks, SageMaker offers flexible distributed training options that adjust to your specific workflows.\n",
    "\n",
    "SageMaker also offers capabilities to prepare models for deployment at the edge. [SageMaker Neo](https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html) is a capability of Amazon SageMaker that enables machine learning models to train once and run anywhere in the cloud and at the edge and [Amazon SageMaker Edge Manager](https://docs.aws.amazon.com/sagemaker/latest/dg/edge.html) provides model management for edge devices so you can optimize, secure, monitor, and maintain machine learning models on fleets of edge devices such as smart cameras, robots, personal computers, and mobile devices.\n",
    "\n",
    "\n",
    "In this notebook we'll train a [**Yolov4**](https://github.com/WongKinYiu/PyTorch_YOLOv4) model on Pytorch using Amazon SageMaker to draw bounding boxes around images and then, compile and package it so that it can be deployed on an edge device(in this case, a [Jetson Xavier](https://developer.nvidia.com/jetpack-sdk-441-archive)).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Pre-requisites\n",
    "\n",
    "Let us start with setting up the pre-requisites for this notebook. First, we will sagemaker and other related libs and then set up the role and buckets and some variables. Note that, we are also specifying the size of the image and also the model size taken as Yolov4s where s stand for small. Check out the [github doc of yolov4](https://github.com/WongKinYiu/PyTorch_YOLOv4) to understand how model sizes differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "from sagemaker.pytorch.estimator import PyTorch\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session=sagemaker.Session()\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "img_size= 416 #reset to 416\n",
    "model_type='tiny' # tiny or ''\n",
    "model_name='yolov4'if model_type=='' else f\"yolov4-{model_type}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Download a public implementation of Yolov4 for Pytorch (Author: Wong Kin Yiu)\n",
    "\n",
    "Now, we will download the PyTorch implementation of Yolov4 from this [repository](https://github.com/WongKinYiu/PyTorch_YOLOv4) which is authored by Wong Kin Yiu. We will place it in a local directory `yolov4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('yolov4'):\n",
    "    !git clone https://github.com/WongKinYiu/PyTorch_YOLOv4 yolov4\n",
    "    !cd yolov4 && git checkout 3c42cbd1b0fa28ad19436d01e0e240404463ff80 && git apply ../mish.patch\n",
    "    !echo 'tensorboard' > yolov4/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1) Train the model by preparing a Python script that will be the entrypoint of the training process\n",
    "\n",
    "Now, we will create a training script to train the Yolov4 model. The training script will wrap the original training scripts and expose the parameters to SageMaker Estimator. The script accepts different arguments which will control the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%writefile yolov4/sagemaker_train.py\n",
    "import sys\n",
    "import subprocess\n",
    "## We need to remove smdebug to avoid the Hook bug https://github.com/awslabs/sagemaker-debugger/issues/401\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"uninstall\", \"-y\", \"smdebug\"])\n",
    "import os\n",
    "import yaml\n",
    "import argparse\n",
    "import torch\n",
    "import shutil\n",
    "import urllib\n",
    "import numpy as np\n",
    "from models.models import Darknet\n",
    "from utils.parse_config import parse_model_cfg\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    \n",
    "    parser.add_argument('--num-classes', type=int, default=80, help='Number of classes')\n",
    "    parser.add_argument('--img-size', type=int, default=640, help='Size of the image')\n",
    "    parser.add_argument('--epochs', type=int, default=1, help='Number of epochs')\n",
    "    parser.add_argument('--batch-size', type=int, default=16, help='Batch size')\n",
    "    parser.add_argument('--adam', action='store_true', help='use torch.optim.Adam() optimizer')\n",
    "    parser.add_argument('--pretrained', action='store_true', help='use pretrained model')\n",
    "    \n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ[\"SM_MODEL_DIR\"], help='Trained model dir')\n",
    "    parser.add_argument('--train', type=str, default=os.environ[\"SM_CHANNEL_TRAIN\"], help='Train path')\n",
    "    parser.add_argument('--train-suffix', type=str, default='', help='Train path suffix')\n",
    "    parser.add_argument('--validation', type=str, default=os.environ[\"SM_CHANNEL_VALIDATION\"], help='Validation path')\n",
    "    parser.add_argument('--validation-suffix', type=str, default='', help='Validation path suffix')\n",
    "    \n",
    "    parser.add_argument('--model-type', type=str, choices=['', 'tiny'], default=\"\", help='Model type')\n",
    "    \n",
    "    # hyperparameters\n",
    "    with open('data/hyp.scratch.yaml', 'r') as f:\n",
    "        hyperparams = yaml.load(f, Loader=yaml.FullLoader)    \n",
    "    for k,v in hyperparams.items():\n",
    "        parser.add_argument(f\"--{k.replace('_', '-')}\", type=float, default=v)\n",
    "    \n",
    "    args,unknown = parser.parse_known_args()\n",
    "    \n",
    "    base_path=os.path.dirname(__file__)\n",
    "    project_dir = os.environ[\"SM_OUTPUT_DATA_DIR\"]\n",
    "\n",
    "    # prepare the hyperparameters metadat\n",
    "    with open(os.path.join(base_path,'data', 'hyp.custom.yaml'), 'w' ) as y:\n",
    "        y.write(yaml.dump({h:vars(args)[h] for h in hyperparams.keys()}))\n",
    "\n",
    "    # prepare the training data metadata\n",
    "    with open(os.path.join(base_path,'data', 'custom.yaml'), 'w') as y:\n",
    "        y.write(yaml.dump({            \n",
    "            'names': [f'class_{i}' for i in range(args.num_classes)],\n",
    "            'train': os.path.join(args.train, args.train_suffix),\n",
    "            'val': os.path.join(args.validation, args.validation_suffix),\n",
    "            'nc': args.num_classes\n",
    "        }))\n",
    "\n",
    "    model_name = \"yolov4\" if len(args.model_type) == 0 else f\"yolov4-{args.model_type}\"\n",
    "    \n",
    "    model_cfg_path = os.path.join(base_path, \"cfg\", f\"{model_name}.cfg\")\n",
    "    model_params = parse_model_cfg(model_cfg_path)\n",
    "    with open(model_cfg_path, 'w') as cfg:\n",
    "        for p in model_params:\n",
    "            if p['type'] == 'yolo':\n",
    "                p['classes'] = args.num_classes\n",
    "            elif p['type'] == 'convolutional' and p['filters'] == 255:\n",
    "                p['filters'] = (args.num_classes + 5) * 3 # adjustment for the classes\n",
    "            for k,v in p.items():\n",
    "                if k==\"type\": cfg.write(f\"[{v}]\\n\")\n",
    "                else:\n",
    "                    if type(v)==np.ndarray:\n",
    "                        v = ','.join(str(i) for i in v.flatten().tolist())\n",
    "                    elif type(v)==list:\n",
    "                        v = ','.join(str(i) for i in v)\n",
    "                    cfg.write(f\"{k}={v}\\n\")\n",
    "    \n",
    "    # run the training script\n",
    "    weights_file=''\n",
    "    if args.pretrained:\n",
    "        weights_file = f'weights/{model_name}.weights'\n",
    "        urllib.request.urlretrieve(\n",
    "            f'https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/{model_name}.weights',\n",
    "            weights_file\n",
    "        )\n",
    "        \n",
    "    train_cmd = [\n",
    "        sys.executable, os.path.join(base_path,'train.py'),\n",
    "        \"--data\", \"custom.yaml\",\n",
    "        \"--hyp\", \"hyp.custom.yaml\",\n",
    "        \"--cfg\", f\"cfg/{model_name}.cfg\",\n",
    "        \"--img\", str(args.img_size),\n",
    "        \"--batch\", str(args.batch_size),\n",
    "        \"--epochs\", str(args.epochs),\n",
    "        \"--logdir\", project_dir,\n",
    "        \"--weights\", weights_file\n",
    "    ]\n",
    "    if args.adam: train_cmd.append(\"--adam\")\n",
    "    subprocess.check_call(train_cmd)\n",
    "        \n",
    "    # tracing and saving the model\n",
    "    inp = torch.rand(1, 3, args.img_size, args.img_size).cpu()\n",
    "    ckpt = torch.load(os.path.join(project_dir, 'exp0', 'weights', 'best.pt'), map_location='cpu')\n",
    "    model = Darknet(f\"cfg/{model_name}.cfg\").cpu()\n",
    "    # do not invoke .eval(). we don't need the detection layer\n",
    "    model.load_state_dict(ckpt['model'], strict=False)\n",
    "    p = model(inp)\n",
    "    model_trace = torch.jit.trace(model, inp)\n",
    "    model_trace.save(os.path.join(args.model_dir, 'model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2) Preparing the dataset\n",
    "\n",
    "Here we'll download a sample dataset [coco128](https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip). We can also replace this step with any other dataset. \n",
    "\n",
    "Just take a look on the labels format and create your dataset scructure following the same standard (COCO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#if not os.path.exists('coco128'):\n",
    "#    !wget -q https://github.com/ultralytics/yolov5/releases/download/v1.0/coco128.zip\n",
    "#    !unzip -q coco128.zip && rm -f coco128.zip\n",
    "print('BBoxes annotation')\n",
    "print('class x_center y_center width height')\n",
    "!head coco128/labels/train2017/2_3-1_thermal_cropped0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.3) Upload the dataset to S3\n",
    "\n",
    "Once the dataset has been downloaded locally, we'll upload the dataset to an S3 bucket created earlier. We are setting up the training and validation dataset s3 locations here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix='data/coco128'\n",
    "!rm -f coco128/labels/train2017.cache\n",
    "train_path = sagemaker_session.upload_data('coco128', key_prefix=f'{prefix}/train')\n",
    "val_path = sagemaker_session.upload_data('coco128', key_prefix=f'{prefix}/val')\n",
    "print(train_path, val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls --recursive s3://sagemaker-eu-west-1-763989535729/data/coco128/train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.4) Prepare the SageMaker Estimator to train the model\n",
    "\n",
    "Now it's time to create an Estimater and train the model with the training script created in earlier step. We are using Pytorch estimator and supplying other arguments in the estimator. Note that we are supplying the `source_dir` so that sagemaker can pick up the training script and other related files from there. Once the estimator is ready, we start the training using the `.fit()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = PyTorch(\n",
    "    'sagemaker_train.py',\n",
    "    source_dir='yolov4',\n",
    "    framework_version='1.7',\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session, \n",
    "    instance_type='ml.p3.2xlarge',    \n",
    "    instance_count=1,\n",
    "    py_version='py3', \n",
    "    hyperparameters={\n",
    "        'epochs': 20, # at least 2 epochs\n",
    "        'batch-size': 8,\n",
    "        'lr0': 0.0001,\n",
    "        \n",
    "        'pretrained': True, # transfer learning\n",
    "        'num-classes': 2,\n",
    "        'img-size': img_size,\n",
    "        'model-type': model_type,\n",
    "        \n",
    "        # the final path needs to point to the images dir\n",
    "        'train-suffix': 'images/train2017',\n",
    "        'validation-suffix': 'images/train2017'\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "estimator.fit({'train': train_path, 'validation': val_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_uri=f'{estimator.output_path}{estimator.latest_training_job.name}/output/model.tar.gz'\n",
    "print('trained model: ' + s3_uri )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Compile your trained model for the edge device\n",
    "\n",
    "Once the model has been traied, we need to compile the model using SageMaker Neo. This step is needed to compile the model for the specific hardware on which this model will be deployed. \n",
    "\n",
    "In this notebook, we will compile a model for [Jetson Xavier Jetpack 4.4.1](https://developer.nvidia.com/jetpack-sdk-441-archive). \n",
    "\n",
    "In case, you want to compile for a different hardware platform, just change the parameters bellow to adjust the target to your own edge device. Also, note, that if you dont have GPU available on the hardware device, then you can comment the `Accelerator` key:value in the `OutputConfig`.\n",
    "\n",
    "The below cell also calls the `describe_compilation_job` API in a loop to wait for the compilation job to complete. In actual applications, it is advisable to setup a cloudwatch event which can notify OR execute the next steps once the compilation job is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "sm_client = boto3.client('sagemaker')\n",
    "compilation_job_name = f'{model_name}-pytorch-{int(time.time()*1000)}'\n",
    "sm_client.create_compilation_job(\n",
    "    CompilationJobName=compilation_job_name,\n",
    "    RoleArn=role,\n",
    "    InputConfig={\n",
    "        'S3Uri': s3_uri,\n",
    "        'DataInputConfig': f'{{\"input\": [1,3,{img_size},{img_size}]}}',\n",
    "        'Framework': 'PYTORCH'\n",
    "    },\n",
    "    OutputConfig={\n",
    "        'S3OutputLocation': f's3://{sagemaker_session.default_bucket()}/{model_name}-pytorch/optimized/',\n",
    "        'TargetPlatform': { \n",
    "            'Os': 'LINUX', \n",
    "            'Arch': 'X86_64'#, # change this to X86_64 if you need\n",
    "            #'Arch': 'ARM64', # change this to X86_64 if you need\n",
    "            #'Accelerator': 'NVIDIA'  # comment this if you don't have an Nvidia GPU\n",
    "        }#,\n",
    "        # Comment or change the following line depending on your edge device\n",
    "        # Jetson Xavier: sm_72; Jetson Nano: sm_53\n",
    "        #'CompilerOptions': '{\"trt-ver\": \"7.1.3\", \"cuda-ver\": \"10.2\", \"gpu-code\": \"sm_53\"}' # Jetpack 4.4.1\n",
    "    },\n",
    "    StoppingCondition={ 'MaxRuntimeInSeconds': 900 }\n",
    ")\n",
    "while True:\n",
    "    resp = sm_client.describe_compilation_job(CompilationJobName=compilation_job_name)    \n",
    "    if resp['CompilationJobStatus'] in ['STARTING', 'INPROGRESS']:\n",
    "        print('Running...')\n",
    "    else:\n",
    "        print(resp['CompilationJobStatus'], compilation_job_name)\n",
    "        break\n",
    "    time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Download the compiled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://github.com/AlexeyAB/darknet/releases/download/darknet_yolo_v4_pre/yolov4-tiny.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tracing and saving the model\n",
    "#import torch\n",
    "#inp = torch.rand(1, 3, 416, 416).cpu()\n",
    "#ckpt = torch.load(\"yolov4-tiny.weights\", map_location='cpu')\n",
    "#model = Darknet(f\"yolov4/cfg/{model_name}.cfg\").cpu()\n",
    "# do not invoke .eval(). we don't need the detection layer\n",
    "#model.load_state_dict(ckpt['model'], strict=False)\n",
    "#p = model(inp)\n",
    "#model_trace = torch.jit.trace(model, inp)\n",
    "#model_trace.save('model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_model_path = f's3://{sagemaker_session.default_bucket()}/{model_name}-pytorch/optimized/model-LINUX_X86_64.tar.gz'\n",
    "\n",
    "!aws s3 cp $output_model_path /tmp/model.tar.gz\n",
    "!rm -rf model_object_detection && mkdir model_object_detection\n",
    "!tar -xzvf /tmp/model.tar.gz -C model_object_detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Run the model locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1) Load the model using the runtime DLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dlr==1.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import dlr\n",
    "\n",
    "# Read the image using OpenCV\n",
    "img = cv2.imread('./coco128/images/train2017/2_3-15_thermal_cropped0.jpg')\n",
    "plt.imshow(img)\n",
    "\n",
    "#img = np.random.rand(1,3,512,512).astype(np.float32)\n",
    "#plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelDLR = dlr.DLRModel('model_object_detection', 'cpu')\n",
    "\n",
    "def predict(model, x):\n",
    "    return model.run(x)\n",
    "\n",
    "confidence_treshold=0.1\n",
    "# Convert the image to the expected network input\n",
    "x = utils.preprocess_img(img, img_size=416)\n",
    "# Run the model and get the predictions\n",
    "preds = predict(modelDLR, x)\n",
    "\n",
    "# Convert the predictions into Detections(bounding boxes, scores and class ids)\n",
    "detections = utils.detect(preds, 0.25, 0.5, True)\n",
    "\n",
    "print('detection.')\n",
    "print(detections)\n",
    "\n",
    "# Iterate over the detections and do what you need to do\n",
    "for top_left_corner,bottom_right_corner, conf, class_id in detections:\n",
    "    if confidence_treshold < 0.1: continue\n",
    "    print( f\"bbox: {top_left_corner},{bottom_right_corner}, score: {conf}, class_id: {class_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Create a SageMaker Edge Manager packaging job\n",
    "\n",
    "Once the model has been compiled, it is time to create an edge manager packaging job. Packaging job take SageMaker Neoâ€“compiled models and make any changes necessary to deploy the model with the inference engine, Edge Manager agent.\n",
    "\n",
    "We need to provide the name used for the Neo compilation job, a name for the packaging job, a role ARN, a name for the model, a model version, and the Amazon S3 bucket URI for the output of the packaging job. Note that Edge Manager packaging job names are case-sensitive.\n",
    "\n",
    "\n",
    "The below cell also calls the `describe_edge_packaging_job` API in a loop to wait for the packaging job to complete. In actual applications, it is advisable to setup a cloudwatch event which can notify OR execute the next steps once the compilation job is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "model_version = '1.0'\n",
    "edge_packaging_job_name=f'{model_name}-pytorch-{int(time.time()*1000)}'\n",
    "resp = sm_client.create_edge_packaging_job(\n",
    "    EdgePackagingJobName=edge_packaging_job_name,\n",
    "    CompilationJobName=compilation_job_name,\n",
    "    ModelName=model_name,\n",
    "    ModelVersion=model_version,\n",
    "    RoleArn=role,\n",
    "    OutputConfig={\n",
    "        'S3OutputLocation': f's3://{bucket_name}/{model_name}'\n",
    "    }\n",
    ")\n",
    "while True:\n",
    "    resp = sm_client.describe_edge_packaging_job(EdgePackagingJobName=edge_packaging_job_name)    \n",
    "    if resp['EdgePackagingJobStatus'] in ['STARTING', 'INPROGRESS']:\n",
    "        print('Running...')\n",
    "    else:\n",
    "        print(resp['EdgePackagingJobStatus'], compilation_job_name)\n",
    "        print('package s3 location: ' + 's3://'+bucket_name+'/'+model_name)\n",
    "        break\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Pre processing + Post processing code\n",
    "After compiling your model, it's time to prepare the application that will use it. In the image bellow you can see the operators that are used by the last layer **YOLOLayer**. When in evaluation mode, this layer applies some operations to merge the two outputs of the network and prepare the predictions for the **Non Maximum Suppression**. In training model, you just have the two raw outputs. \n",
    "\n",
    "Given we're using a pruned version (training mode) of the network, you need to apply some post processing code to your predictions.\n",
    "\n",
    "<table style=\"border: 1px solid black; border-collapse: collapse;\" border=3 cellpadding=0 cellspacing=0>\n",
    "    <tr style=\"border: 1px solid black;\">\n",
    "        <td style=\"text-align: center; border-right: 1px solid;\" align=\"center\"><b>WITH DETECTION (evaluation mode)</b></td>\n",
    "        <td style=\"text-align: center;\" align=\"center\"><b>NO DETECTION (training mode)</b></td>\n",
    "    </tr>    \n",
    "    <tr style=\"border: 1px solid black; border-right: 1px solid;\">\n",
    "        <td width=\"75%\" style=\"border-right: 1px solid;\">\n",
    "            <img src=\"imgs/yolov4_detection.png\"/>\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"imgs/yolov4_no_detection.png\"/>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "All the operations required by this process can be found in the script **[Utils](utils.py)**.\n",
    "#### WARNING: run the next cell and copy the output to your **utils.py** before running the application. It is necessary to adjust the anchors and scores for the correct yolov4 version\n",
    "\n",
    "\n",
    "Here it is an example of how to use the code:\n",
    "```Python\n",
    "import utils\n",
    "\n",
    "### your code here\n",
    "### def predict(model, x):...\n",
    "\n",
    "confidence_treshold=0.1\n",
    "# Read the image using OpenCV\n",
    "img = cv2.imread('dog.jpg')\n",
    "# Convert the image to the expected network input\n",
    "x = utils.preprocess_img(img, img_size=416)\n",
    "# Run the model and get the predictions\n",
    "preds = model(x)\n",
    "# Convert the predictions into Detections(bounding boxes, scores and class ids)\n",
    "detections = utils.detect(preds, 0.25, 0.5, True)\n",
    "# Iterate over the detections and do what you need to do\n",
    "for top_left_corner,bottom_right_corner, conf, class_id in detections:\n",
    "    if confidence_treshold < 0.1: continue\n",
    "    print( f\"bbox: {top_left_corner},{bottom_right_corner}, score: {conf}, class_id: {class_id}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This code parses the .cfg file of the version you're using and\n",
    "## Prints the correct anchors/stride. Copy the output to the 'utils.py' file used by your application\n",
    "import numpy as np\n",
    "\n",
    "cfg_filename=f'yolov4/cfg/{model_name}.cfg'\n",
    "yolo_layer=False\n",
    "yolo_layers = []\n",
    "\n",
    "for i in open(cfg_filename, 'r').readlines(): # read the .cfg file\n",
    "    i = i.strip()\n",
    "    if len(i) == 0 or i.startswith('#'): continue # ignore empty lines and comments\n",
    "    elif i.startswith('[') and i.endswith(']'): # header\n",
    "        if i.lower().replace(' ', '') == '[yolo]':\n",
    "            yolo_layer = True # yolo layer\n",
    "            yolo_layers.append({})\n",
    "    elif yolo_layer: # properties of the layer\n",
    "        k,v = [a.strip() for a in i.split('=')] # split line into key, value\n",
    "        if k == 'anchors': # parse anchors\n",
    "            anchors = np.array([int(j.strip()) for j in v.split(',')])\n",
    "            yolo_layers[-1]['anchors'] = anchors.reshape((len(anchors)//2, 2))\n",
    "        elif k == 'mask': # parse mask\n",
    "            yolo_layers[-1]['mask'] = np.array([int(j.strip()) for j in v.split(',')])\n",
    "\n",
    "stride = [8, 16, 32, 64, 128]  # P3, P4, P5, P6, P7 strides\n",
    "if model_type == 'tiny':  # P5, P4, P3 strides\n",
    "    stride = [32, 16, 8]\n",
    "\n",
    "print(\"##### Copy the following lines to your util.py\")\n",
    "print(f\"anchors = {[l['anchors'][l['mask']].tolist() for l in yolo_layers]}\")\n",
    "print(f\"stride = {stride}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Done !!\n",
    "\n",
    "And we are done with all the steps needed to prepare the model for deploying to edge. The model package is avaialble in S3 and can be taken from there to deploy it to edge device. Now you need to move over to your edge device and download and setup edge manager agent(runtime), model and other related artifacts on the device. Please check out the [documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/edge.html) for detailed steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7) Setup Edge Manager Server\n",
    "### 7.1) First you need to prepare and run a Cloudformation template to create the infrastructure required for running the Edge Manager\n",
    "Run the next cell, click on the link it will create and don't forget to ack the capability in the Cloudformation screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from IPython import display\n",
    "\n",
    "s3_client = boto3.client('s3')\n",
    "sts_client = boto3.client('sts')\n",
    "\n",
    "session = boto3.session.Session()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = session.region_name\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "account_id = sts_client.get_caller_identity()[\"Account\"]\n",
    "\n",
    "stack_name='experiments'\n",
    "fleet_name=f'micro-device-fleet-{stack_name}'\n",
    "device_name=f'device0-micro-device-fleet-{stack_name}'\n",
    "policy_name=f'MicroDeviceFleetPolicy-{stack_name}'\n",
    "iot_policy_name=f'SageMakerEdge-micro-device-fleet-{stack_name}'\n",
    "\n",
    "template_url=f'https://s3.amazonaws.com/spock.cloud/ml-edge/micro-device-fleet.yml'\n",
    "template_params={\n",
    "    'stackName': stack_name,\n",
    "    'param_BucketName': bucket_name,\n",
    "    'param_SageMakerRoleArn': role\n",
    "}\n",
    "params = \"&\".join([f\"{k}={v}\" for k,v in template_params.items()])\n",
    "cfn_quick_link=f\"https://{region}.console.aws.amazon.com/cloudformation/home?region={region}#/stacks/create/review?templateURL={template_url}&{params}\"\n",
    "\n",
    "display.HTML(f'<a href=\"{cfn_quick_link}\"> >>> Click <strong>here</strong> to launch the Cloudformation Stack, but don\\'t forget to ack the following capability << </a> ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7.2) Install some dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get -y update && apt-get -y install build-essential procps\n",
    "!pip install -U numpy sysv_ipc boto3 grpcio-tools grpcio protobuf sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 7.3) Download and prepare the SageMaker Edge Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7.3.1) Download/unpack the agent for X86_64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import os\n",
    "import tarfile\n",
    "import io\n",
    "import stat\n",
    "\n",
    "sm_client = boto3.client('sagemaker')\n",
    "\n",
    "agent_version = '1.20210820.e20fa3a'\n",
    "agent_pkg_bucket = 'sagemaker-edge-release-store-us-west-2-linux-x64'\n",
    "\n",
    "if not os.path.isdir('agent'):\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # Download and install SageMaker Edge Manager\n",
    "    agent_pkg_key = f'Releases/{agent_version}/{agent_version}.tgz'\n",
    "    # get the agent package\n",
    "    with io.BytesIO() as file:\n",
    "        s3_client.download_fileobj(agent_pkg_bucket, agent_pkg_key, file)\n",
    "        file.seek(0)\n",
    "        # Extract the files\n",
    "        tar = tarfile.open(fileobj=file)\n",
    "        tar.extractall('agent')\n",
    "        tar.close()\n",
    "        # Adjust the permissions\n",
    "        os.chmod('agent/bin/sagemaker_edge_agent_binary', stat.S_IXUSR|stat.S_IWUSR|stat.S_IXGRP|stat.S_IWGRP)\n",
    "    \n",
    "    # by using protoc, we can generate stubs (client api) for connecting to the agent and invoking its API\n",
    "    if not os.path.isdir('app'): os.mkdir('app')\n",
    "    !python3 -m grpc_tools.protoc --proto_path=agent/docs/api --python_out=app/ --grpc_python_out=app/ agent/docs/api/agent.proto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7.3.2) Create the certificates if needed\n",
    "These are IoT certificates, used by the agent to communicate with the cloud when needed: to send heartbeats and or capture data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "cert_root_dir=os.path.join('agent', 'certificates', 'root')\n",
    "cert_iot_dir=os.path.join('agent', 'certificates', 'iot')\n",
    "if not os.path.isdir(cert_root_dir): os.makedirs(cert_root_dir)\n",
    "if not os.path.isdir(cert_iot_dir): os.makedirs(cert_iot_dir)\n",
    "\n",
    "# assume a different role (created by the CFN you executed at the beginning for IoT ops)\n",
    "role_arn = f\"arn:aws:iam::{account_id}:role/MicroDeviceFleetRole-{stack_name}\"\n",
    "temp_role = boto3.client('sts').assume_role(RoleArn=role_arn, RoleSessionName=\"MicroDeviceFleetSession2\")\n",
    "credentials = temp_role['Credentials']\n",
    "\n",
    "iot_client = boto3.client(\n",
    "    'iot',\n",
    "    aws_access_key_id=credentials['AccessKeyId'],\n",
    "    aws_secret_access_key=credentials['SecretAccessKey'],\n",
    "    aws_session_token=credentials['SessionToken']\n",
    ")\n",
    "\n",
    "if not os.path.isfile(os.path.join(cert_iot_dir, 'device0_key.pem')):\n",
    "    print('Creating a new certificate')\n",
    "    cert_meta = iot_client.create_keys_and_certificate(setAsActive=True)\n",
    "    cert_arn = cert_meta['certificateArn']\n",
    "    # attach the certificates to the policy and to the thing\n",
    "    iot_client.attach_policy(policyName=policy_name, target=cert_arn)\n",
    "    \n",
    "    with open(os.path.join(cert_iot_dir, 'device0_cert.pem'), 'w') as c: c.write(cert_meta['certificatePem'])\n",
    "    with open(os.path.join(cert_iot_dir, 'device0_key.pem'),  'w') as c: c.write(cert_meta['keyPair']['PrivateKey'])\n",
    "    with open(os.path.join(cert_iot_dir, 'device0_pub.pem'),  'w') as c: c.write(cert_meta['keyPair']['PublicKey'])\n",
    "    \n",
    "    # get root certs\n",
    "    urllib.request.urlretrieve('https://www.amazontrust.com/repository/AmazonRootCA1.pem', os.path.join(cert_root_dir, 'AmazonRootCA1.pem'))\n",
    "    # this certificate validates the edge manage package\n",
    "    s3_client.download_file(\n",
    "        Bucket=agent_pkg_bucket, \n",
    "        Key=f'Certificates/{region}/{region}.pem', \n",
    "        Filename=os.path.join(cert_root_dir, f\"{region}.pem\")\n",
    "    )        \n",
    "    # adjust the permissions of the files\n",
    "    os.chmod(os.path.join(cert_root_dir, 'AmazonRootCA1.pem'), stat.S_IRUSR|stat.S_IRGRP)\n",
    "    os.chmod(os.path.join(cert_root_dir, f\"{region}.pem\"), stat.S_IRUSR|stat.S_IRGRP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 7.3.3) Create the Agent config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "cred_host=iot_client.describe_endpoint(endpointType='iot:CredentialProvider')['endpointAddress']\n",
    "agent_params = {\n",
    "    \"sagemaker_edge_core_device_name\": device_name,\n",
    "    \"sagemaker_edge_core_device_fleet_name\": fleet_name,\n",
    "    \"sagemaker_edge_core_capture_data_buffer_size\": 30,\n",
    "    \"sagemaker_edge_core_capture_data_batch_size\": 10,\n",
    "    \"sagemaker_edge_core_capture_data_push_period_seconds\": 4,\n",
    "    \"sagemaker_edge_core_folder_prefix\": \"micro-device-fleet-data\",\n",
    "    \"sagemaker_edge_core_region\": region,\n",
    "    \"sagemaker_edge_core_root_certs_path\": \"./agent/certificates/root\",\n",
    "    \"sagemaker_edge_provider_aws_ca_cert_file\":\"./agent/certificates/root/AmazonRootCA1.pem\",\n",
    "    \"sagemaker_edge_provider_aws_cert_file\": os.path.join(\".\", cert_iot_dir, 'device0_cert.pem'),\n",
    "    \"sagemaker_edge_provider_aws_cert_pk_file\":os.path.join(\".\", cert_iot_dir, 'device0_key.pem'),\n",
    "    \"sagemaker_edge_provider_aws_iot_cred_endpoint\": f\"https://{cred_host}/role-aliases/{iot_policy_name}/credentials\",\n",
    "    \"sagemaker_edge_provider_provider\": \"Aws\",\n",
    "    \"sagemaker_edge_provider_provider_path\" : os.path.abspath(\"./agent/lib/libprovider_aws.so\"),\n",
    "    \"sagemaker_edge_provider_s3_bucket_name\": bucket_name,\n",
    "    \"sagemaker_edge_core_capture_data_destination\": \"Cloud\"\n",
    "}\n",
    "conf_dir = os.path.join('agent', 'conf')\n",
    "conf_file_name='config_edge_device0.json'\n",
    "if not os.path.isdir(conf_dir): os.makedirs(conf_dir)\n",
    "with open(os.path.join(conf_dir, conf_file_name), 'w') as conf:\n",
    "    conf_file = json.dumps(agent_params, indent=4)\n",
    "    conf.write(conf_file)\n",
    "    print(conf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 8) Run the agent in background\n",
    "The agent is a simple application that will run on your OS. Let's use subprocess to create a background process with the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "channel_path='/tmp/agent_dev'\n",
    "if os.path.exists(channel_path): os.remove(channel_path)\n",
    "cmd = f'./agent/bin/sagemaker_edge_agent_binary -c agent/conf/{conf_file_name} -a {channel_path}'\n",
    "print(cmd)\n",
    "if not os.path.exists('agent/logs'): os.makedirs('agent/logs')\n",
    "logs = open(\"agent/logs/agent0.log\", \"+w\")\n",
    "proc = subprocess.Popen(cmd.split(' '), stdout=logs)\n",
    "time.sleep(2)\n",
    "!ps aux --cols 300|grep sagemaker_edge_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 9) Test the model, using SageMaker Edge Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(1, 'app')\n",
    "import numpy as np\n",
    "import grpc\n",
    "\n",
    "# Loading the stubs - agent python client\n",
    "import agent_pb2 as agent\n",
    "import agent_pb2_grpc as agent_grpc\n",
    "import edge_agent_utils as edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connecting to the agent\n",
    "channel = grpc.insecure_channel(f'unix://{channel_path}' )\n",
    "client = agent_grpc.AgentStub(channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_alias='image-classification'\n",
    "\n",
    "## Loading a model in the agent\n",
    "edge.load_model(client, model_alias, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Validate the model with a an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick and dirty parser for the labels\n",
    "labels = {}\n",
    "for l in open('labels.txt', 'r').read().splitlines():\n",
    "    l = l.strip().replace('{', '').replace('}', '')\n",
    "    l = l[:-1] if l.endswith(',') else l\n",
    "    cls_id,label = [t.strip() for t in l.split(':')]\n",
    "    labels[int(cls_id)] = label[1:-1] # remove the single quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "img_size=224\n",
    "# load the image and make it squared if needed\n",
    "img = cv2.cvtColor(cv2.imread('cat.jpg'), cv2.COLOR_BGR2RGB)\n",
    "h,w,c = img.shape\n",
    "if w!=h: # pad the image and make it square\n",
    "    sqr_size = max(h,w)\n",
    "    sqr_img = np.zeros((sqr_size, sqr_size, c), dtype=np.uint8)\n",
    "    sqr_img[:h, :w],img = img,sqr_img\n",
    "# resize the image to the expected size+transform it to pytorch/imagenet format\n",
    "x = cv2.resize(img, (img_size, img_size)).astype(np.float32) / 255.0\n",
    "# normalize\n",
    "x -= [0.485, 0.456, 0.406]\n",
    "x /= [0.229, 0.224, 0.225]\n",
    "x = x.transpose(2,0,1) # HWC --> CHW\n",
    "c,h,w = x.shape\n",
    "x = x.reshape(1,c,h,w) # CHW --> NCHW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = edge.predict(client, model_alias, x)\n",
    "idx = np.argmax(y)\n",
    "print(f\"Class id: {idx}, Score: {y[0][idx]}, Label: {labels[idx]}\")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Done, you can stop the agent now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop the agent\n",
    "proc.kill()"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.6 Python 3.6 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-west-1:470317259841:image/pytorch-1.6-gpu-py36-cu110-ubuntu18.04-v3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
